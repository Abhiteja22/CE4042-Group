{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from operator import itemgetter\n",
    "from skimage import exposure\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ast import literal_eval as make_tuple\n",
    "\n",
    "#Create folder to store the serialized objects\n",
    "import os\n",
    "if not os.path.isdir('./serialized/gender/'):\n",
    "    os.mkdir('./serialized/gender/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A method to serialize the python object to .pkl file for reusability\n",
    "def save_obj(obj,name):\n",
    "\tbase_path = './serialized/gender/'\n",
    "\twith open(base_path+name + '.pkl', 'wb') as f:\n",
    "\t\tpickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A method to deserialize the python object from .pkl file, returning it (up to the program to handle where and which type of variable it is loaded to)\n",
    "def load_obj(name):\n",
    "\twith open(name + '.pkl', 'rb') as f:\n",
    "\t\treturn pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to integer encode the age\n",
    "#Note that there is problem in the original labels of age where (25,32) == (25,23)\n",
    "#This is to account for that error as well\n",
    "\n",
    "#This is for future work on gender-based age classification, not much useful now\n",
    "def get_age_range_id(age_tuple):\n",
    "\tage_ranges = [(0,2),(4,6),(8,13),(15,20),(25,32),(38,43),(48,53),(60,100)]\n",
    "\tdiff_tuple = []\n",
    "    \n",
    "\tif age_tuple:\n",
    "\t\tfor r in age_ranges:\n",
    "\t\t\tx = tuple(np.subtract(r,age_tuple))\n",
    "\t\t\tx = tuple(np.absolute(x))\n",
    "\t\t\tdiff_tuple.append(x)\n",
    "\n",
    "\tmin_index = diff_tuple.index(min(diff_tuple, key=itemgetter(1)))\n",
    "\treturn min_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test the tricky case see if the correct index (from 0 is returned)\n",
    "test_tuple = (25,23)\n",
    "get_age_range_id((25,23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resizing images\n",
    "width = 256\n",
    "height= 256\n",
    "\n",
    "#Folds indexes there are 5 of them so 0-4\n",
    "fold_indexes = list(range(5))\n",
    "\n",
    "#original, unsanitised fold txt prefix\n",
    "fold_txt_prefix_path = 'Folds/original_txt_files/'\n",
    "#prefix for sanitised, 80/20 splitted labels in each folds\n",
    "train_test_splitted_fold_path = './Folds/train_test_splitted/'\n",
    "#prefix to store csv files, both for training and testing and for cross validation\n",
    "cv_fold_csv_prefix_path = \"Folds/\"\n",
    "train_test_csv_path = \"Folds/\"\n",
    "#image prefix path\n",
    "image_prefix_path = 'Adience/aligned/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in fold 0\n",
      "Splitted original fold 0 in 80:20 train:test ratio.\n",
      "Reading in fold 1\n",
      "Splitted original fold 1 in 80:20 train:test ratio.\n",
      "Reading in fold 2\n",
      "Splitted original fold 2 in 80:20 train:test ratio.\n",
      "Reading in fold 3\n",
      "Splitted original fold 3 in 80:20 train:test ratio.\n",
      "Reading in fold 4\n",
      "Splitted original fold 4 in 80:20 train:test ratio.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"For original, uncleaned 5 folds, we wished to obtained the sanitised individual folds labels\n",
    "Each individual fold is splitted into 80:20 for train and test.\n",
    "The train subset will be either used as validate set or combined with other folds' train subset to form the train set\n",
    "The test subset will be added up together to form test set.\n",
    "Overall is train/val/test split\"\"\"\n",
    "for fold in fold_indexes:\n",
    "    print(\"Reading in fold %s\"%fold)\n",
    "    df = pd.read_csv(fold_txt_prefix_path+\"fold_%s_data.txt\"%fold, sep=\"\\t\")\n",
    "    #Clean up nasty Nan, None, what not?\n",
    "    df = df[df['age']!='None']\n",
    "    df = df[df['age']!=' ']\n",
    "    df = df[df['gender'].notnull()]\n",
    "    df = df[df['gender']!=' ']\n",
    "    df = df[df['gender']!='u']\n",
    "    #Split once cleaned\n",
    "    train_df, test_df = train_test_split(df,test_size = 0.2,shuffle=True,random_state=10)\n",
    "    #Export this sanitised labels\n",
    "    train_df.to_csv(train_test_splitted_fold_path+\"cv_fold_%s_train.csv\"%fold,index=False)\n",
    "    test_df.to_csv(train_test_splitted_fold_path+\"cv_fold_%s_test.csv\"%fold,index=False)\n",
    "    print(\"Splitted original fold %s in 80:20 train:test ratio.\"%fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the train-test set for optimal model training and testing...\n"
     ]
    }
   ],
   "source": [
    "#Create the test set, train set labels by combining the 80% portions together and 20% portions together\n",
    "print(\"Creating the train-test set for optimal model training and testing...\")\n",
    "train_df = pd.concat([pd.read_csv(train_test_splitted_fold_path+\"cv_fold_%s_train.csv\"%fold) for fold in fold_indexes])\n",
    "test_df  = pd.concat([pd.read_csv(train_test_splitted_fold_path+\"cv_fold_%s_test.csv\"%fold) for fold in fold_indexes])\n",
    "\n",
    "train_df.to_csv(train_test_csv_path+\"train_set.csv\",index=False)\n",
    "test_df.to_csv(train_test_csv_path+\"test_set.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4] 0\n",
      "[0, 2, 3, 4] 1\n",
      "[0, 1, 3, 4] 2\n",
      "[0, 1, 2, 4] 3\n",
      "[0, 1, 2, 3] 4\n"
     ]
    }
   ],
   "source": [
    "#Create train subsets and validation set for each folds\n",
    "save_fold_path = './Folds/'\n",
    "for val_fold in fold_indexes:\n",
    "    #get the train_subset_fold id to merge\n",
    "    train_subset_folds = [fold for fold in fold_indexes if fold!=val_fold]\n",
    "    print(train_subset_folds,val_fold)\n",
    "    \n",
    "    #generate the train_subset and and the validation df\n",
    "    train_subset_df = pd.concat([pd.read_csv(train_test_splitted_fold_path+\"cv_fold_%s_train.csv\"%train_fold) for train_fold in train_subset_folds])\n",
    "    val_df = pd.read_csv(train_test_splitted_fold_path+\"cv_fold_%s_train.csv\"%val_fold)\n",
    "    \n",
    "    #export to csv\n",
    "    train_subset_df.to_csv(cv_fold_csv_prefix_path+\"cv_fold_%s_train_subset.csv\"%val_fold,index=False)\n",
    "    val_df.to_csv(cv_fold_csv_prefix_path+\"cv_fold_%s_val.csv\"%val_fold,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For easiness in moving files to servers for training, pickle all images...\n",
    "dataset_to_be_pickled = ['train_set','test_set',\n",
    "                        'cv_fold_0_train_subset','cv_fold_0_val',\n",
    "                        'cv_fold_1_train_subset','cv_fold_1_val',\n",
    "                        'cv_fold_2_train_subset','cv_fold_2_val',\n",
    "                        'cv_fold_3_train_subset','cv_fold_3_val',\n",
    "                        'cv_fold_4_train_subset','cv_fold_4_val',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling dataset train_set.csv ...\n",
      "Progress Update: Done with train_set.csv\n",
      "No. Images: 13960, No. Gender: 13960, No. Ages: 13960\n",
      "\n",
      "Pickling dataset test_set.csv ...\n",
      "Progress Update: Done with test_set.csv\n",
      "No. Images: 3492, No. Gender: 3492, No. Ages: 3492\n",
      "\n",
      "Pickling dataset cv_fold_0_train_subset.csv ...\n",
      "Progress Update: Done with cv_fold_0_train_subset.csv\n",
      "No. Images: 10764, No. Gender: 10764, No. Ages: 10764\n",
      "\n",
      "Pickling dataset cv_fold_0_val.csv ...\n",
      "Progress Update: Done with cv_fold_0_val.csv\n",
      "No. Images: 3196, No. Gender: 3196, No. Ages: 3196\n",
      "\n",
      "Pickling dataset cv_fold_1_train_subset.csv ...\n",
      "Progress Update: Done with cv_fold_1_train_subset.csv\n",
      "No. Images: 11083, No. Gender: 11083, No. Ages: 11083\n",
      "\n",
      "Pickling dataset cv_fold_1_val.csv ...\n",
      "Progress Update: Done with cv_fold_1_val.csv\n",
      "No. Images: 2877, No. Gender: 2877, No. Ages: 2877\n",
      "\n",
      "Pickling dataset cv_fold_2_train_subset.csv ...\n",
      "Progress Update: Done with cv_fold_2_train_subset.csv\n",
      "No. Images: 11461, No. Gender: 11461, No. Ages: 11461\n",
      "\n",
      "Pickling dataset cv_fold_2_val.csv ...\n",
      "Progress Update: Done with cv_fold_2_val.csv\n",
      "No. Images: 2499, No. Gender: 2499, No. Ages: 2499\n",
      "\n",
      "Pickling dataset cv_fold_3_train_subset.csv ...\n",
      "Progress Update: Done with cv_fold_3_train_subset.csv\n",
      "No. Images: 11328, No. Gender: 11328, No. Ages: 11328\n",
      "\n",
      "Pickling dataset cv_fold_3_val.csv ...\n",
      "Progress Update: Done with cv_fold_3_val.csv\n",
      "No. Images: 2632, No. Gender: 2632, No. Ages: 2632\n",
      "\n",
      "Pickling dataset cv_fold_4_train_subset.csv ...\n",
      "Progress Update: Done with cv_fold_4_train_subset.csv\n",
      "No. Images: 11204, No. Gender: 11204, No. Ages: 11204\n",
      "\n",
      "Pickling dataset cv_fold_4_val.csv ...\n",
      "Progress Update: Done with cv_fold_4_val.csv\n",
      "No. Images: 2756, No. Gender: 2756, No. Ages: 2756\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Serializing of objects (images and labels) into a consolidate file for use on Google drive\n",
    "for dataset in dataset_to_be_pickled:\n",
    "    print(\"Pickling dataset %s.csv ...\"%dataset)\n",
    "    df = pd.read_csv(cv_fold_csv_prefix_path+dataset+'.csv')\n",
    "    #Arrays of attributes and labels\n",
    "    cleaned_images = []\n",
    "    genders = []\n",
    "    ages = []\n",
    "    \n",
    "    #Looping through each images and do selection, then do preprocessing\n",
    "    for index, row in df.iterrows():\n",
    "        yaw = row['fiducial_yaw_angle']\n",
    "        gender = row['gender']\n",
    "        age = row['age']\n",
    "        \n",
    "        #Get necessary information to construct the path to the image\n",
    "        image_folder = row['user_id']\n",
    "        image_file = row['original_image']\n",
    "        face_id = row['face_id']\n",
    "\n",
    "        age_tuple = make_tuple(age)\n",
    "        age_id = get_age_range_id(age_tuple)\n",
    "        #Assemble the path to image\n",
    "        path_to_image = image_prefix_path+image_folder+'/landmark_aligned_face.'+str(face_id)+'.'+image_file\n",
    "        #Preprocess image\n",
    "        image_to_preprocess = Image.open(path_to_image)\n",
    "        #Resize image and convert to array form for storage\n",
    "        image_processed = image_to_preprocess.resize((width, height), PIL.Image.LANCZOS)\n",
    "        image_processed_array = np.array(image_processed)\n",
    "\n",
    "        #Integer encode the gender\n",
    "        if (gender == \"m\"):\n",
    "            gender_label_integer = 0\n",
    "        else:\n",
    "            gender_label_integer = 1\n",
    "\n",
    "        #Prepare all the datastructures for storage in serialization\n",
    "        cleaned_images.append(image_processed_array)\n",
    "        genders.append(gender_label_integer)\n",
    "        ages.append(age_id)\n",
    "        \n",
    "        #Otherwise just move to the next image already!\n",
    "    \n",
    "    #Once done, save to serialized pickle form\n",
    "    print(\"Progress Update: Done with %s.csv\"%dataset)          \n",
    "    print ('No. Images: %i, No. Gender: %i, No. Ages: %i' % (len(cleaned_images), len(genders), len(ages)))            \n",
    "    print ('')\n",
    "    \n",
    "    this_dict = {'dataset_name': dataset, 'images': cleaned_images, 'genders': genders, 'ages': ages}\n",
    "    save_obj(this_dict,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
